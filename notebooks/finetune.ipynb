{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7196c50a",
   "metadata": {},
   "source": [
    "# Finetuning Insurance Conditions Model\n",
    "\n",
    "Detta Colab-notebook guidar dig genom:\n",
    "1. Installera beroenden  \n",
    "2. Ladda dataset  \n",
    "3. Tokenisering & f√∂rberedelser  \n",
    "4. Finetuning med Hugging Face ü§ó Transformers  \n",
    "5. Utv√§rdering  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e1619",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Installera beroenden (k√∂r bara en g√•ng)\n",
    "!pip install transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1358276",
   "metadata": {},
   "source": [
    "## Ladda och inspektera dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671d6c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# L√§s in ditt JSONL‚Äìdataset\n",
    "ds = load_dataset(\"json\", data_files=\"../data/dataset.jsonl\", split=\"train\")\n",
    "print(ds[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c0b5f",
   "metadata": {},
   "source": [
    "## 2. Initiera modell och tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2e908",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"birgermoell/t5-base-swedish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd4f50",
   "metadata": {},
   "source": [
    "## 3. F√∂rbehandling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad16ce0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(ex):\n",
    "    # Tokenisera input och output, s√§tt labels fr√•n output\n",
    "    inputs = tokenizer(ex[\"input\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    outputs = tokenizer(ex[\"output\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# K√∂r preprocess p√• hela datasetet\n",
    "tokenized = ds.map(preprocess, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d61257",
   "metadata": {},
   "source": [
    "## 4. Finetuning med ü§ó Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a5b539",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/t5-mvp\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    logging_dir=\"../models/logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    ")\n",
    "\n",
    "# Starta tr√§ningen\n",
    "trainer.train()\n",
    "\n",
    "# Spara slutmodell och tokenizer\n",
    "model.save_pretrained(\"../models/t5-final\")\n",
    "tokenizer.save_pretrained(\"../models/t5-final\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ae4bd",
   "metadata": {},
   "source": [
    "## 5. Enkel utv√§rdering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f66ddf3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Exempel p√• inference\n",
    "from transformers import pipeline\n",
    "gen = pipeline(\"text2text-generation\", model=\"../models/t5-final\", tokenizer=\"../models/t5-final\")\n",
    "\n",
    "sample = ds[1]\n",
    "print(\"INPUT:\", sample[\"input\"])\n",
    "print(\"PRED:\", gen(sample[\"input\"], max_length=128)[0][\"generated_text\"])\n",
    "print(\"TRUE:\", sample[\"output\"])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
